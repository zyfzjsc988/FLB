{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T18:40:26.140562Z",
     "start_time": "2018-08-08T18:40:25.277569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# train data\n",
    "summary_folder_path = '../../datasets/train_data/summary'\n",
    "mining_folder_path = '../../datasets/train_data/data_mining'\n",
    "result_path = '../../datasets/results'\n",
    "games = { 'baseball':'/baseball/mlb',\n",
    "         'iceball': '/iceball/nhl',\n",
    "         'soccer_champion':'/soccer/champ_league',\n",
    "         'scoccer_england':'/soccer/epl',\n",
    "         'soccer_major':'/soccer/majorleague'}\n",
    "# folder name is games, result name is games_results.csv\n",
    "\n",
    "games_list = ['baseball','iceball']\n",
    "soccer_list = ['soccer_champion','scoccer_england','soccer_major']\n",
    "\n",
    "soccer_features_name = ['maximum', 'minimum', 'ave', 'ave_normalized', \n",
    "            'start', 'start_normalized', 'end', 'end_normalized', 'start2end',\n",
    "            'start2end_normalized', 'start2max', 'start2max_normalized', 'start2min', 'start2min_normalized', \n",
    "            'std', 'no_price', 'length', \n",
    "            'up_num', 'down_num', 'up_rate', 'down_rate', 'duration', \n",
    "            'bias_max', 'bias_min', 'bias_ave', 'bias_st', 'draw_odds','away_odds']\n",
    "\n",
    "games_features_name = ['maximum', 'minimum', 'ave', 'ave_normalized', \n",
    "            'start', 'start_normalized', 'end', 'end_normalized', 'start2end',\n",
    "            'start2end_normalized', 'start2max', 'start2max_normalized', 'start2min', 'start2min_normalized', \n",
    "            'std', 'no_price', 'length', \n",
    "            'up_num', 'down_num', 'up_rate', 'down_rate', 'duration', \n",
    "            'bias_max', 'bias_min', 'bias_ave', 'bias_st']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. models\n",
    "logistic regression,\n",
    "KNN, \n",
    "Naive Bayes,\n",
    "DT,\n",
    "RF,\n",
    "AdaBoost,\n",
    "Gradient tree boosting,\n",
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T18:49:26.771121Z",
     "start_time": "2018-08-08T18:49:25.025122Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput: \\ntraining: matrix of features, target: two or three target\\nvalidate: matrix of features, target: two or three target\\n\\noutput: \\nacc, auc, or others\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "input: \n",
    "training: matrix of features, target: two or three target\n",
    "validate: matrix of features, target: two or three target\n",
    "\n",
    "output: \n",
    "acc, auc, or others\n",
    "\"\"\"\n",
    "\n",
    "#  train ml models\n",
    "# input: features and labels\n",
    "# output: all acc\n",
    "def predict_soccer_result(features, label):\n",
    "    acc_dic = {}\n",
    "#     knn\n",
    "    knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsClassifier(n_neighbors=60))\n",
    "    acc_dic['knn'] = cross_val_score(knn, features, label, cv=5).max()\n",
    "#     logistic regression\n",
    "    logreg = make_pipeline(preprocessing.StandardScaler(), LogisticRegression(C=0.3))\n",
    "    acc_dic['logistic_regression'] = cross_val_score(logreg, features, label, cv=5).max()\n",
    "#     naive bayes\n",
    "    bayes = make_pipeline(preprocessing.StandardScaler(), BernoulliNB())\n",
    "    acc_dic['bayes'] = cross_val_score(bayes, features, label, cv=5).max()\n",
    "#     decision tree   \n",
    "    dt = make_pipeline(preprocessing.StandardScaler(), DecisionTreeClassifier(random_state=0,max_depth=2))\n",
    "    acc_dic['decision_tree'] = cross_val_score(dt, features, label, cv=5).max()\n",
    "#     random forest\n",
    "    rf = make_pipeline(preprocessing.StandardScaler(), RandomForestClassifier(max_depth=5, random_state=0))\n",
    "    acc_dic['random_forest'] = cross_val_score(rf, features, label, cv=5).max()\n",
    "#     adaboost\n",
    "    adaboost = make_pipeline(preprocessing.StandardScaler(), AdaBoostClassifier(n_estimators=230))\n",
    "    acc_dic['AdaBoost'] = cross_val_score(adaboost, features, label, cv=5).max()\n",
    "#     gradient boosting\n",
    "    gb = make_pipeline(preprocessing.StandardScaler(),\n",
    "                        GradientBoostingClassifier(n_estimators=240, \n",
    "                                                   learning_rate=1.0,\n",
    "                                                   max_depth=1, \n",
    "                                                   random_state=0))\n",
    "    acc_dic['GradientBoosting'] = cross_val_score(gb, features, label, cv=5).max()    \n",
    "\n",
    "    return acc_dic\n",
    "\n",
    "\n",
    "def regression_next(features,label):\n",
    "    mse_dic = {}\n",
    "#     lasso\n",
    "    alpha = 0.02\n",
    "    lasso = make_pipeline(preprocessing.StandardScaler(), Lasso(alpha=alpha))\n",
    "    mse_dic['Lasso'] = cross_val_score(lasso, features, label, scoring='r2', cv=5).max()\n",
    "#     elasticnet\n",
    "    elasticnet = make_pipeline(preprocessing.StandardScaler(), ElasticNet(alpha=alpha, l1_ratio=0.7))\n",
    "    mse_dic['ElasticNet'] = cross_val_score(elasticnet, features, label, scoring='r2', cv=5).max()   \n",
    "#     bayes regression\n",
    "    bayesianRidge = make_pipeline(preprocessing.StandardScaler(), BayesianRidge())\n",
    "    mse_dic['BayesianRidge'] = cross_val_score(bayesianRidge, features, label, scoring='r2', cv=5).max()     \n",
    "#     svm regression\n",
    "    svr = make_pipeline(preprocessing.StandardScaler(), svm.SVR(C=0.01))\n",
    "    mse_dic['svm'] = cross_val_score(svr, features, label, scoring='r2', cv=5).max()   \n",
    "#     KNN regression\n",
    "    knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsRegressor(n_neighbors=60))\n",
    "    mse_dic['KNN'] = cross_val_score(knn, features, label, scoring='r2', cv=5).max()     \n",
    "#     decision tree   \n",
    "    dt = make_pipeline(preprocessing.StandardScaler(), DecisionTreeRegressor(random_state=0,max_depth=2))\n",
    "    mse_dic['decision_tree'] = cross_val_score(dt, features, label, scoring='r2', cv=5).max()\n",
    "#     random forest\n",
    "    rf = make_pipeline(preprocessing.StandardScaler(), RandomForestRegressor(max_depth=5, random_state=0))\n",
    "    mse_dic['random_forest'] = cross_val_score(rf, features, label, scoring='r2', cv=5).max()\n",
    "#     adaboost\n",
    "    adaboost = make_pipeline(preprocessing.StandardScaler(), AdaBoostRegressor(n_estimators=230))\n",
    "    mse_dic['AdaBoost'] = cross_val_score(adaboost, features, label,scoring='r2', cv=5).max()\n",
    "#     gradient boosting\n",
    "    gb = make_pipeline(preprocessing.StandardScaler(),\n",
    "                        GradientBoostingRegressor(n_estimators=240, \n",
    "                                                   learning_rate=1.0,\n",
    "                                                   max_depth=1, \n",
    "                                                   random_state=0))\n",
    "    mse_dic['GradientBoosting'] = cross_val_score(gb, features, label, scoring='r2', cv=5).max()   \n",
    "#     MLP\n",
    "    mlp = make_pipeline(preprocessing.StandardScaler(), MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                       activation='logistic',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=1000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.002))\n",
    "    mse_dic['MLP'] = cross_val_score(mlp, features, label, scoring='r2', cv=5).max()  \n",
    "    \n",
    "    return mse_dic\n",
    "\n",
    "def regression_next_mse(features,label):\n",
    "    mse_dic = {}\n",
    "    score = 'neg_mean_squared_error'\n",
    "#     lasso\n",
    "    alpha = 0.02\n",
    "    lasso = make_pipeline(preprocessing.StandardScaler(), Lasso(alpha=alpha))\n",
    "    mse_dic['Lasso'] = -cross_val_score(lasso, features, label, scoring=score, cv=5).min()\n",
    "#     elasticnet\n",
    "    elasticnet = make_pipeline(preprocessing.StandardScaler(), ElasticNet(alpha=alpha, l1_ratio=0.7))\n",
    "    mse_dic['ElasticNet'] = -cross_val_score(elasticnet, features, label, scoring=score, cv=5).min() \n",
    "#     bayes regression\n",
    "    bayesianRidge = make_pipeline(preprocessing.StandardScaler(), BayesianRidge())\n",
    "    mse_dic['BayesianRidge'] = -cross_val_score(bayesianRidge, features, label,scoring=score, cv=5).min()  \n",
    "#     svm regression\n",
    "    svr = make_pipeline(preprocessing.StandardScaler(), svm.SVR(C=0.01))\n",
    "    mse_dic['svm'] = -cross_val_score(svr, features, label, scoring=score, cv=5).min()  \n",
    "#     KNN regression\n",
    "    knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsRegressor(n_neighbors=60))\n",
    "    mse_dic['KNN'] = -cross_val_score(knn, features, label, scoring=score, cv=5).min()\n",
    "#     decision tree   \n",
    "    dt = make_pipeline(preprocessing.StandardScaler(), DecisionTreeRegressor(random_state=0,max_depth=2))\n",
    "    mse_dic['decision_tree'] = -cross_val_score(dt, features, label, scoring=score, cv=5).min()\n",
    "#     random forest\n",
    "    rf = make_pipeline(preprocessing.StandardScaler(), RandomForestRegressor(max_depth=5, random_state=0))\n",
    "    mse_dic['random_forest'] = -cross_val_score(rf, features, label, scoring=score, cv=5).min()\n",
    "#     adaboost\n",
    "    adaboost = make_pipeline(preprocessing.StandardScaler(), AdaBoostRegressor(n_estimators=230))\n",
    "    mse_dic['AdaBoost'] = -cross_val_score(adaboost, features, label,scoring=score, cv=5).min()\n",
    "#     gradient boosting\n",
    "    gb = make_pipeline(preprocessing.StandardScaler(),\n",
    "                        GradientBoostingRegressor(n_estimators=240, \n",
    "                                                   learning_rate=1.0,\n",
    "                                                   max_depth=1, \n",
    "                                                   random_state=0))\n",
    "    mse_dic['GradientBoosting'] = - cross_val_score(gb, features, label, scoring=score, cv=5).min() \n",
    "#     MLP\n",
    "    mlp = make_pipeline(preprocessing.StandardScaler(), MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                       activation='logistic',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=1000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.002))\n",
    "    mse_dic['MLP'] = -cross_val_score(mlp, features, label, scoring=score, cv=5).min()\n",
    "    \n",
    "    return mse_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T09:20:26.323785Z",
     "start_time": "2018-08-08T09:20:26.124783Z"
    },
    "code_folding": [
     0,
     12
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_guess(sec_no,series):\n",
    "    count = 0\n",
    "    for i in series.index.tolist():\n",
    "        if series.loc[i,'end'+sec_no] > series.loc[i,'draw_odds'+sec_no] and \\\n",
    "        series.loc[i,'end'+sec_no] > series.loc[i,'away_odds'+sec_no]:\n",
    "            if int(series.loc[i,'home']) == 1:\n",
    "                count +=1\n",
    "        else:\n",
    "            if int(series.loc[i,'home']) == 0:\n",
    "                count +=1\n",
    "    return count/len(unbiased)\n",
    "\n",
    "def summary(sec_name,sec_no):\n",
    "    results = pd.DataFrame()\n",
    "    dic = predict_soccer_result(unbiased[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,unbiased)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'unbiased'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    dic = predict_soccer_result(hourly_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,hourly_corrected)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'hourly_corrected'\n",
    "    results = results.append(pred)\n",
    "    \n",
    "    dic = predict_soccer_result(daily_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,daily_corrected)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'daily_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    dic = predict_soccer_result(global_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,global_corrected)\n",
    "    pred = pd.Series(dic) \n",
    "    pred.name = 'global_corrected'\n",
    "    results = results.append(pred)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T09:20:29.062780Z",
     "start_time": "2018-08-08T09:20:27.660781Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surface\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# soccer\n",
    "game_name = 'soccer'\n",
    "\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "# all target home, draw , away\n",
    "target = unbiased[['home','draw','away']]\n",
    "target['target'] = (target['home']+target['draw']*2+target['away']*3).apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T21:39:19.546140Z",
     "start_time": "2018-08-07T21:26:58.842842Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# risk analysis\n",
    "# high risk index / low risk index\n",
    "\n",
    "risk_value = 'no_risk'\n",
    "labels = target['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in soccer_features_name])\n",
    "all_features.extend([i+'_2' for i in soccer_features_name])\n",
    "all_features.extend([i+'_3' for i in soccer_features_name])\n",
    "all_features.extend([i+'_all' for i in soccer_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T22:25:21.090597Z",
     "start_time": "2018-08-07T22:15:43.950029Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surface\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3180"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased[['home','draw','away']]\n",
    "target['target'] = (target['home']+target['draw']*2+target['away']*3).apply(int)\n",
    "\n",
    "\n",
    "risk_value = 'high_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']<0.7) & (unbiased['start_all']>0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = target['home'].apply(int).values\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in soccer_features_name])\n",
    "all_features.extend([i+'_2' for i in soccer_features_name])\n",
    "all_features.extend([i+'_3' for i in soccer_features_name])\n",
    "all_features.extend([i+'_all' for i in soccer_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T22:30:15.850553Z",
     "start_time": "2018-08-07T22:25:22.759590Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surface\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased[['home','draw','away']]\n",
    "target['target'] = (target['home']+target['draw']*2+target['away']*3).apply(int)\n",
    "\n",
    "\n",
    "risk_value = 'low_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']>=0.7) | (unbiased['start_all']<=0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = target['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in soccer_features_name])\n",
    "all_features.extend([i+'_2' for i in soccer_features_name])\n",
    "all_features.extend([i+'_3' for i in soccer_features_name])\n",
    "all_features.extend([i+'_all' for i in soccer_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iceball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T22:33:13.203041Z",
     "start_time": "2018-08-07T22:33:13.104039Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_guess(sec_no,series):\n",
    "    count = 0\n",
    "    for i in series.index.tolist():\n",
    "        if series.loc[i,'end'+sec_no] >= 0.5:\n",
    "            if int(series.loc[i,'home']) == 1:\n",
    "                count +=1\n",
    "        else:\n",
    "            if int(series.loc[i,'home']) == 0:\n",
    "                count +=1\n",
    "    return count/len(unbiased)\n",
    "\n",
    "def summary(sec_name,sec_no):\n",
    "    results = pd.DataFrame()\n",
    "    dic = predict_soccer_result(unbiased[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,unbiased)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'unbiased'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    dic = predict_soccer_result(hourly_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,hourly_corrected)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'hourly_corrected'\n",
    "    results = results.append(pred)\n",
    "    \n",
    "    dic = predict_soccer_result(daily_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,daily_corrected)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'daily_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    dic = predict_soccer_result(global_corrected[sec_name].values, labels)\n",
    "    dic['odds_guess'] = random_guess(sec_no,global_corrected)\n",
    "    pred = pd.Series(dic) \n",
    "    pred.name = 'global_corrected'\n",
    "    results = results.append(pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T22:40:26.275679Z",
     "start_time": "2018-08-07T22:38:52.114065Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'low_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']>=0.7) | (unbiased['start_all']<=0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T23:01:45.097724Z",
     "start_time": "2018-08-07T22:41:08.934543Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6663"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'high_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']<0.7) & (unbiased['start_all']>0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T23:22:41.549646Z",
     "start_time": "2018-08-07T23:01:46.530642Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'no_risk'\n",
    "\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T23:58:53.876901Z",
     "start_time": "2018-08-07T23:22:42.839636Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'no_risk'\n",
    "\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T00:29:18.311961Z",
     "start_time": "2018-08-07T23:58:55.953900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11743"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'high_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']<0.7) & (unbiased['start_all']>0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T00:30:59.388939Z",
     "start_time": "2018-08-08T00:29:19.775903Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n",
      "sec3 to predict:\n",
      "all to predict:\n",
      "all features to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# all target home, draw , away\n",
    "target = unbiased['home']\n",
    "\n",
    "risk_value = 'low_risk'\n",
    "high_risk = unbiased[((unbiased['start_all']>=0.7) | (unbiased['start_all']<=0.3))].index.tolist()\n",
    "len(high_risk)\n",
    "unbiased = unbiased.loc[high_risk]\n",
    "hourly_corrected = hourly_corrected.loc[high_risk]\n",
    "daily_corrected = daily_corrected.loc[high_risk]\n",
    "global_corrected = global_corrected.loc[high_risk]\n",
    "target = target.loc[high_risk]\n",
    "labels = unbiased['home'].apply(int).values\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# # sec3\n",
    "print('sec3 to predict:')\n",
    "tag = '_3'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "# all\n",
    "print('all to predict:')\n",
    "tag = '_all'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary(sec1_name,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'.csv')\n",
    "\n",
    "print('all features to predict:')\n",
    "tag = '_all'\n",
    "all_features =[]\n",
    "all_features.extend([i+'_1' for i in games_features_name])\n",
    "all_features.extend([i+'_2' for i in games_features_name])\n",
    "all_features.extend([i+'_3' for i in games_features_name])\n",
    "all_features.extend([i+'_all' for i in games_features_name])\n",
    "results_1 = summary(all_features,tag)\n",
    "results_1.to_csv(result_path+'/home_win/'+game_name+'/'+risk_value+tag+'features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### predict direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T10:08:27.015883Z",
     "start_time": "2018-08-08T10:00:57.322882Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "def direction_find(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    elif x==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def summary_direction_results(sec_name,sec_no,next_no):\n",
    "    results = pd.DataFrame()\n",
    "    target = (unbiased['end'+next_no] - unbiased['end'+sec_no]).apply(direction_find)\n",
    "    dic = predict_soccer_result(unbiased[sec_name].values, target)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'unbiased'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    \n",
    "#     target = (hourly_corrected['end'+next_no] - hourly_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = predict_soccer_result(hourly_corrected[sec_name].values, target)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'hourly_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (daily_corrected['end'+next_no] - daily_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = predict_soccer_result(daily_corrected[sec_name].values, target)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'daily_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (global_corrected['end'+next_no] - global_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = predict_soccer_result(global_corrected[sec_name].values, target)\n",
    "    pred = pd.Series(dic) \n",
    "    pred.name = 'global_corrected'\n",
    "    results = results.append(pred)\n",
    "    return results\n",
    "\n",
    "# soccer\n",
    "game_name = 'soccer'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T10:16:29.662299Z",
     "start_time": "2018-08-08T10:08:27.493890Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "# iceball\n",
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T10:28:43.664412Z",
     "start_time": "2018-08-08T10:16:30.059260Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "# baseball\n",
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_direction_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/direction/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression: next period price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T11:43:16.763644Z",
     "start_time": "2018-08-08T11:39:22.412631Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "def summary_price_results(sec_name,sec_no,next_no):\n",
    "    results = pd.DataFrame()\n",
    "    target = unbiased['end'+sec_no]\n",
    "    dic = regression_next(unbiased[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'unbiased'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    \n",
    "#     target = (hourly_corrected['end'+next_no] - hourly_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next(hourly_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'hourly_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (daily_corrected['end'+next_no] - daily_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next(daily_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'daily_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (global_corrected['end'+next_no] - global_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next(global_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic) \n",
    "    pred.name = 'global_corrected'\n",
    "    results = results.append(pred)\n",
    "    return results\n",
    "\n",
    "# soccer\n",
    "game_name = 'soccer'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T11:52:24.119216Z",
     "start_time": "2018-08-08T11:47:49.530197Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T11:59:34.464197Z",
     "start_time": "2018-08-08T11:52:24.782200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T18:53:08.908726Z",
     "start_time": "2018-08-08T18:50:56.768176Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "# mse as metrics\n",
    "def summary_price_results(sec_name,sec_no,next_no):\n",
    "    results = pd.DataFrame()\n",
    "    target = unbiased['end'+sec_no]\n",
    "    dic = regression_next_mse(unbiased[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'unbiased'\n",
    "    results = results.append(pred)\n",
    "\n",
    "    \n",
    "#     target = (hourly_corrected['end'+next_no] - hourly_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next_mse(hourly_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'hourly_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (daily_corrected['end'+next_no] - daily_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next_mse(daily_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic)\n",
    "    pred.name = 'daily_corrected'\n",
    "    results = results.append(pred)\n",
    "\n",
    "#     target = (global_corrected['end'+next_no] - global_corrected['end'+sec_no]).apply(direction_find)\n",
    "    dic = regression_next_mse(global_corrected[sec_name].values, target.values)\n",
    "    pred = pd.Series(dic) \n",
    "    pred.name = 'global_corrected'\n",
    "    results = results.append(pred)\n",
    "    return results\n",
    "\n",
    "# soccer\n",
    "game_name = 'soccer'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in soccer_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T18:56:17.937169Z",
     "start_time": "2018-08-08T18:53:24.254220Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'iceball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T19:05:11.458181Z",
     "start_time": "2018-08-08T18:56:18.212174Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec1 to predict:\n",
      "sec2 to predict:\n"
     ]
    }
   ],
   "source": [
    "game_name = 'baseball'\n",
    "unbiased = pd.read_csv(mining_folder_path+'/'+game_name+'_unbiased.csv').iloc[:,1:].fillna(0)\n",
    "hourly_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_hourly_corrected.csv').iloc[:,1:].fillna(0)\n",
    "daily_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_daily_corrected.csv').iloc[:,1:].fillna(0)\n",
    "global_corrected = pd.read_csv(mining_folder_path+'/'+game_name+'_global_corrected.csv').iloc[:,1:].fillna(0)\n",
    "\n",
    "# sec1\n",
    "print('sec1 to predict:')\n",
    "tag = '_1'\n",
    "sec_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_2')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')\n",
    "\n",
    "# sec2\n",
    "print('sec2 to predict:')\n",
    "tag = '_2'\n",
    "sec1_name = [i+tag for i in games_features_name]\n",
    "results_1 = summary_price_results(sec_name,tag,'_3')\n",
    "results_1.to_csv(result_path+'/price_mse/'+game_name+tag+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "379px",
    "left": "718.267px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
