{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    digger the features\n",
    "including:\n",
    "- minimum/ \n",
    "- max / \n",
    "- ave / ave normalized\n",
    "- start / start normalized\n",
    "- end / end normalized\n",
    "- difference between start to end / normalized\n",
    "- difference between start to max / normalized\n",
    "- difference between start to min / normalized\n",
    "- std\n",
    "- the number of different price\n",
    "- up number / rate\n",
    "- down number /rate\n",
    "- length\n",
    "- duration\n",
    "- Bias: max, min, ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:12:07.257096Z",
     "start_time": "2018-08-06T15:12:07.243592Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# train data\n",
    "summary_folder_path = '../../datasets/train_data/summary'\n",
    "mining_folder_path = '../../datasets/train_data/data_mining'\n",
    "result_folder_path = '../../datasets/rawdata'\n",
    "games = { 'baseball':'/baseball/mlb',\n",
    "         'iceball': '/iceball/nhl',\n",
    "         'soccer_champion':'/soccer/champ_league',\n",
    "         'scoccer_england':'/soccer/epl',\n",
    "         'soccer_major':'/soccer/majorleague'}\n",
    "# folder name is games, result name is games_results.csv\n",
    "\n",
    "games_list = ['baseball','iceball']\n",
    "soccer_list = ['soccer_champion','scoccer_england','soccer_major']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:18:59.898011Z",
     "start_time": "2018-08-06T15:18:59.338011Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diggers(df,home_name):\n",
    "    # input is the frame, price_series_name\n",
    "    # output is summary\n",
    "    if len(df) < 1:\n",
    "        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    odds_series = df[home_name].tolist()\n",
    "    \n",
    "#     odds\n",
    "    draw_odds = df.loc[df.index[-1],'draw'],\n",
    "    away_odds = df.loc[df.index[-1],'away']\n",
    "    \n",
    "    # length\n",
    "    length = len(odds_series)\n",
    "    # duration\n",
    "    duration = df.iloc[-1,0]\n",
    "    # bias\n",
    "    bias = abs(df[home_name] - df['home']).tolist()\n",
    "    bias_max = max(bias), \n",
    "    bias_min = min(bias), \n",
    "    bias_ave = np.mean(np.array([bias]), axis=1)[0]\n",
    "    bias_std = np.std(np.array([bias]), axis=1)[0]\n",
    "    # maximum # minimum\n",
    "    maximum = max(odds_series)\n",
    "    minimum = min(odds_series)\n",
    "    range_ = maximum - minimum\n",
    "    if range_ == 0:\n",
    "        return [maximum, maximum, maximum, 1, maximum, 1, maximum, 1, 0,0, 0, 0, 0, 0, 0, 1, length, \n",
    "            0, 0, 0, 0, duration, bias_max, bias_min, bias_ave, bias_std, draw_odds, away_odds]\n",
    "    #ave\n",
    "    ave = np.mean(np.array([odds_series]), axis=1)[0]\n",
    "    ave_normalized = (ave - minimum)/range_\n",
    "    #start\n",
    "    start = odds_series[-1]\n",
    "    start_normalized = (start - minimum)/range_\n",
    "    # end \n",
    "    end = odds_series[0]\n",
    "    end_normalized = (end - minimum)/range_\n",
    "    # difference\n",
    "    start2end = end - start\n",
    "    start2end_normalized = (start2end - minimum)/range_\n",
    "    start2max = maximum - start\n",
    "    start2max_normalized = (start2max - minimum)/range_\n",
    "    start2min = minimum - start\n",
    "    start2min_normalized = (start2min - minimum)/range_\n",
    "    # std\n",
    "    std = np.std(np.array([odds_series]), axis=1)[0]\n",
    "    # number of price\n",
    "    no_price = len(set(odds_series))\n",
    "    # tendency\n",
    "    up_num = 0\n",
    "    down_num = 0\n",
    "    for i in range(length-1):\n",
    "        if odds_series[i] < odds_series[i+1]:\n",
    "            up_num += 1\n",
    "        elif odds_series[i] > odds_series[i+1]:\n",
    "            down_num += 1\n",
    "        else:\n",
    "            pass\n",
    "    up_rate = up_num/(up_num+down_num)\n",
    "    down_rate = down_num/(up_num+down_num)\n",
    "\n",
    "    \n",
    "    return [maximum, minimum, ave, ave_normalized, \n",
    "            start, start_normalized, end, end_normalized, start2end,\n",
    "            start2end_normalized, start2max, start2max_normalized, start2min, start2min_normalized, \n",
    "            std, no_price, length, \n",
    "            up_num, down_num, up_rate, down_rate, duration, \n",
    "            bias_max, bias_min, bias_ave, bias_std, draw_odds, away_odds]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:36:13.536921Z",
     "start_time": "2018-08-06T15:21:26.673157Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer_champion\n",
      "scoccer_england\n",
      "soccer_major\n"
     ]
    }
   ],
   "source": [
    "unbiased = pd.DataFrame()\n",
    "hourly_corrected = pd.DataFrame()\n",
    "daily_corrected = pd.DataFrame()\n",
    "global_corrected = pd.DataFrame()\n",
    "\n",
    "features_name = ['maximum', 'minimum', 'ave', 'ave_normalized', \n",
    "            'start', 'start_normalized', 'end', 'end_normalized', 'start2end',\n",
    "            'start2end_normalized', 'start2max', 'start2max_normalized', 'start2min', 'start2min_normalized', \n",
    "            'std', 'no_price', 'length', \n",
    "            'up_num', 'down_num', 'up_rate', 'down_rate', 'duration', \n",
    "            'bias_max', 'bias_min', 'bias_ave', 'bias_st', 'draw_odds','away_odds']\n",
    "\n",
    "for game_name in soccer_list:\n",
    "    print(game_name)\n",
    "    results = pd.read_csv(result_folder_path+games[game_name]+'_results.csv').set_index('Unnamed: 0')\n",
    "    index_list = [i[:-4] for i in os.listdir(summary_folder_path + games[game_name])]\n",
    "    for index in index_list:\n",
    "        series_df = pd.read_csv(summary_folder_path + games[game_name]+'/'+str(index)+'.csv')\n",
    "        if len(series_df) < 7:\n",
    "            continue\n",
    "        # sec 3\n",
    "        sec3 = series_df[series_df['duration']<= int(series_df.iloc[-1,0]/3)] # sec 3\n",
    "        # sec 2\n",
    "        sec2 = series_df[(series_df['duration']<= int(series_df.iloc[-1,0]/3*2)) \\\n",
    "                  & (series_df['duration']> int(series_df.iloc[-1,0]/3)) ] #sec 2\n",
    "        # sec 1\n",
    "        sec1 = series_df[series_df['duration']> int(series_df.iloc[-1,0]/3*2)] # sec 1        \n",
    "        \n",
    "        index = int(index)\n",
    "        if results.loc[index,'home_score'] ==  results.loc[index,'away_score'] :\n",
    "            win = {'draw':1,'home':0,'away':0}\n",
    "        elif results.loc[index,'home_score'] >  results.loc[index,'away_score'] :\n",
    "            win = {'draw':0,'home':1,'away':0}\n",
    "        else:\n",
    "            win = {'draw':0,'home':0,'away':1}\n",
    "        \n",
    "#         these four frame will be digger by different series\n",
    "        series_name = 'home'\n",
    "        features = diggers(series_df,series_name)\n",
    "        feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "        features = diggers(sec1,series_name)\n",
    "        feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "        features = diggers(sec2,series_name)\n",
    "        feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "        features = diggers(sec3,series_name)\n",
    "        feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "        \n",
    "        features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "        unbiased = unbiased.append(features, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        series_name = 'hourly_corrected'\n",
    "        features = diggers(series_df,series_name)\n",
    "        feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "        features = diggers(sec1,series_name)\n",
    "        feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "        features = diggers(sec2,series_name)\n",
    "        feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "        features = diggers(sec3,series_name)\n",
    "        feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "        \n",
    "        features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "        hourly_corrected = hourly_corrected.append(features, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        series_name = 'daily_corrected'\n",
    "        features = diggers(series_df,series_name)\n",
    "        feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "        features = diggers(sec1,series_name)\n",
    "        feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "        features = diggers(sec2,series_name)\n",
    "        feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "        features = diggers(sec3,series_name)\n",
    "        feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "        \n",
    "        features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "        daily_corrected = daily_corrected.append(features, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        series_name = 'global_corrected'\n",
    "        features = diggers(series_df,series_name)\n",
    "        feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "        features = diggers(sec1,series_name)\n",
    "        feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "        features = diggers(sec2,series_name)\n",
    "        feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "        features = diggers(sec3,series_name)\n",
    "        feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "        \n",
    "        features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "        global_corrected = global_corrected.append(features, ignore_index=True)\n",
    "\n",
    "unbiased.to_csv(mining_folder_path+'\\soccer_unbiased.csv')\n",
    "hourly_corrected.to_csv(mining_folder_path+'\\soccer_hourly_corrected.csv')\n",
    "daily_corrected.to_csv(mining_folder_path+'\\soccer_daily_corrected.csv')\n",
    "global_corrected.to_csv(mining_folder_path+'\\soccer_global_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:45:38.610304Z",
     "start_time": "2018-08-06T15:45:38.266310Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diggers(df,home_name):\n",
    "    # input is the frame, price_series_name\n",
    "    # output is summary\n",
    "    if len(df) < 1:\n",
    "        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    odds_series = df[home_name].tolist()\n",
    "    \n",
    "    \n",
    "    # length\n",
    "    length = len(odds_series)\n",
    "    # duration\n",
    "    duration = df.iloc[-1,0]\n",
    "    # bias\n",
    "    bias = abs(df[home_name] - df['home']).tolist()\n",
    "    bias_max = max(bias), \n",
    "    bias_min = min(bias), \n",
    "    bias_ave = np.mean(np.array([bias]), axis=1)[0]\n",
    "    bias_std = np.std(np.array([bias]), axis=1)[0]\n",
    "    # maximum # minimum\n",
    "    maximum = max(odds_series)\n",
    "    minimum = min(odds_series)\n",
    "    range_ = maximum - minimum\n",
    "    if range_ == 0:\n",
    "        return [maximum, maximum, maximum, 1, maximum, 1, maximum, 1, 0,0, 0, 0, 0, 0, 0, 1, length, \n",
    "            0, 0, 0, 0, duration, bias_max, bias_min, bias_ave, bias_std]\n",
    "    #ave\n",
    "    ave = np.mean(np.array([odds_series]), axis=1)[0]\n",
    "    ave_normalized = (ave - minimum)/range_\n",
    "    #start\n",
    "    start = odds_series[-1]\n",
    "    start_normalized = (start - minimum)/range_\n",
    "    # end \n",
    "    end = odds_series[0]\n",
    "    end_normalized = (end - minimum)/range_\n",
    "    # difference\n",
    "    start2end = end - start\n",
    "    start2end_normalized = (start2end - minimum)/range_\n",
    "    start2max = maximum - start\n",
    "    start2max_normalized = (start2max - minimum)/range_\n",
    "    start2min = minimum - start\n",
    "    start2min_normalized = (start2min - minimum)/range_\n",
    "    # std\n",
    "    std = np.std(np.array([odds_series]), axis=1)[0]\n",
    "    # number of price\n",
    "    no_price = len(set(odds_series))\n",
    "    # tendency\n",
    "    up_num = 0\n",
    "    down_num = 0\n",
    "    for i in range(length-1):\n",
    "        if odds_series[i] < odds_series[i+1]:\n",
    "            up_num += 1\n",
    "        elif odds_series[i] > odds_series[i+1]:\n",
    "            down_num += 1\n",
    "        else:\n",
    "            pass\n",
    "    up_rate = up_num/(up_num+down_num)\n",
    "    down_rate = down_num/(up_num+down_num)\n",
    "\n",
    "    \n",
    "    return [maximum, minimum, ave, ave_normalized, \n",
    "            start, start_normalized, end, end_normalized, start2end,\n",
    "            start2end_normalized, start2max, start2max_normalized, start2min, start2min_normalized, \n",
    "            std, no_price, length, \n",
    "            up_num, down_num, up_rate, down_rate, duration, \n",
    "            bias_max, bias_min, bias_ave, bias_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-06T15:46:39.983Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unbiased = pd.DataFrame()\n",
    "hourly_corrected = pd.DataFrame()\n",
    "daily_corrected = pd.DataFrame()\n",
    "global_corrected = pd.DataFrame()\n",
    "\n",
    "features_name = ['maximum', 'minimum', 'ave', 'ave_normalized', \n",
    "            'start', 'start_normalized', 'end', 'end_normalized', 'start2end',\n",
    "            'start2end_normalized', 'start2max', 'start2max_normalized', 'start2min', 'start2min_normalized', \n",
    "            'std', 'no_price', 'length', \n",
    "            'up_num', 'down_num', 'up_rate', 'down_rate', 'duration', \n",
    "            'bias_max', 'bias_min', 'bias_ave', 'bias_st']\n",
    "\n",
    "game_name = 'baseball'\n",
    "results = pd.read_csv(result_folder_path+games[game_name]+'_results.csv').set_index('Unnamed: 0')\n",
    "index_list = [i[:-4] for i in os.listdir(summary_folder_path + games[game_name])]\n",
    "for index in index_list:\n",
    "    series_df = pd.read_csv(summary_folder_path + games[game_name]+'/'+str(index)+'.csv')\n",
    "    if len(series_df) < 7:\n",
    "        continue\n",
    "    # sec 3\n",
    "    sec3 = series_df[series_df['duration']<= int(series_df.iloc[-1,0]/3)] # sec 3\n",
    "    # sec 2\n",
    "    sec2 = series_df[(series_df['duration']<= int(series_df.iloc[-1,0]/3*2)) \\\n",
    "              & (series_df['duration']> int(series_df.iloc[-1,0]/3)) ] #sec 2\n",
    "    # sec 1\n",
    "    sec1 = series_df[series_df['duration']> int(series_df.iloc[-1,0]/3*2)] # sec 1        \n",
    "\n",
    "    index = int(index)\n",
    "    if results.loc[index,'home_score'] >  results.loc[index,'away_score'] :\n",
    "        win = {'home':1,'away':0}\n",
    "    else:\n",
    "        win = {'home':0,'away':1}\n",
    "\n",
    "#         these four frame will be digger by different series\n",
    "    series_name = 'home'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    unbiased = unbiased.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    series_name = 'hourly_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    hourly_corrected = hourly_corrected.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "    series_name = 'daily_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    daily_corrected = daily_corrected.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "    series_name = 'global_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    global_corrected = global_corrected.append(features, ignore_index=True)\n",
    "\n",
    "unbiased.to_csv(mining_folder_path+'\\baseball_unbiased.csv')\n",
    "hourly_corrected.to_csv(mining_folder_path+'\\baseball_hourly_corrected.csv')\n",
    "daily_corrected.to_csv(mining_folder_path+'\\baseball_daily_corrected.csv')\n",
    "global_corrected.to_csv(mining_folder_path+'\\baseball_global_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-06T15:47:03.811Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unbiased = pd.DataFrame()\n",
    "hourly_corrected = pd.DataFrame()\n",
    "daily_corrected = pd.DataFrame()\n",
    "global_corrected = pd.DataFrame()\n",
    "\n",
    "features_name = ['maximum', 'minimum', 'ave', 'ave_normalized', \n",
    "            'start', 'start_normalized', 'end', 'end_normalized', 'start2end',\n",
    "            'start2end_normalized', 'start2max', 'start2max_normalized', 'start2min', 'start2min_normalized', \n",
    "            'std', 'no_price', 'length', \n",
    "            'up_num', 'down_num', 'up_rate', 'down_rate', 'duration', \n",
    "            'bias_max', 'bias_min', 'bias_ave', 'bias_st']\n",
    "\n",
    "game_name = 'iceball'\n",
    "results = pd.read_csv(result_folder_path+games[game_name]+'_results.csv').set_index('Unnamed: 0')\n",
    "index_list = [i[:-4] for i in os.listdir(summary_folder_path + games[game_name])]\n",
    "for index in index_list:\n",
    "    series_df = pd.read_csv(summary_folder_path + games[game_name]+'/'+str(index)+'.csv')\n",
    "    if len(series_df) < 7:\n",
    "        continue\n",
    "    # sec 3\n",
    "    sec3 = series_df[series_df['duration']<= int(series_df.iloc[-1,0]/3)] # sec 3\n",
    "    # sec 2\n",
    "    sec2 = series_df[(series_df['duration']<= int(series_df.iloc[-1,0]/3*2)) \\\n",
    "              & (series_df['duration']> int(series_df.iloc[-1,0]/3)) ] #sec 2\n",
    "    # sec 1\n",
    "    sec1 = series_df[series_df['duration']> int(series_df.iloc[-1,0]/3*2)] # sec 1        \n",
    "\n",
    "    index = int(index)\n",
    "    if results.loc[index,'home_score'] >  results.loc[index,'away_score'] :\n",
    "        win = {'home':1,'away':0}\n",
    "    else:\n",
    "        win = {'home':0,'away':1}\n",
    "\n",
    "#         these four frame will be digger by different series\n",
    "    series_name = 'home'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    unbiased = unbiased.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    series_name = 'hourly_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    hourly_corrected = hourly_corrected.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "    series_name = 'daily_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    daily_corrected = daily_corrected.append(features, ignore_index=True)\n",
    "\n",
    "\n",
    "    series_name = 'global_corrected'\n",
    "    features = diggers(series_df,series_name)\n",
    "    feature_dic_all = dict(zip([i+'_all' for i in features_name],features))\n",
    "    features = diggers(sec1,series_name)\n",
    "    feature_dic_1 = dict(zip([i+'_1' for i in features_name],features))\n",
    "    features = diggers(sec2,series_name)\n",
    "    feature_dic_2 = dict(zip([i+'_2' for i in features_name],features))\n",
    "    features = diggers(sec3,series_name)\n",
    "    feature_dic_3 = dict(zip([i+'_3' for i in features_name],features))\n",
    "\n",
    "    features = {**feature_dic_all,**feature_dic_1,**feature_dic_2,**feature_dic_3,**win}\n",
    "    global_corrected = global_corrected.append(features, ignore_index=True)\n",
    "\n",
    "unbiased.to_csv(mining_folder_path+'\\iceball_unbiased.csv')\n",
    "hourly_corrected.to_csv(mining_folder_path+'\\iceball_hourly_corrected.csv')\n",
    "daily_corrected.to_csv(mining_folder_path+'\\iceball_daily_corrected.csv')\n",
    "global_corrected.to_csv(mining_folder_path+'\\iceball_global_corrected.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
